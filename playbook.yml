### main Lustre playbook
---
# Main Lustre Playbook

- name: Setup Lustre on test servers 
  hosts: all
  become: true
  become_user: ec2-user
  gather_facts: false

  # stop ansible from hanging
  any_errors_fatal: true
  # stop ansible if can not log on.
  ignore_unreachable: true

  #### set roles for playbook
  ## update python to python3.8
  roles:
    - role: Set_SSH_Vars

  vars:
    data_device_name: "/dev/nvme1n1"  # Lust
    lustre_device_name: "dev/nvme2n1" # ZFS drive for Lustre device_name = "/dev/sdz"
    lustre_pool_name: "Lust_pool"
    mount_point: "/mnt/data"
    lustre_point: "/mnt/lustre"
    ansible_command_timeout: 60  # timeout if ansible gets stuck
    mounted_devices: "{{ ansible_mounts | map(attribute='device') }}"

  pre_tasks:
  
  ### Set vault file and fetch secret vars from it
    - name: Include vault
      ansible.builtin.include_vars:
        file: Terraform_Vault

  tasks:
    ### I need to key to be on the bastion host so it can copy file to the other servers.
    - name: Copy ssh-key to non-bastion hosts
      ansible.builtin.copy:
        src: "{{ ssh_key_location }}"
        dest: "/home/ec2-user/.ssh/lustretest"
        owner: ec2-user
        mode: '0600'
      when: inventory_hostname != bastion_host

    ### shwo Current IP ans host status
    - name: Print Public IP Address
      debug:
        msg:
          - "Public IP of {{ inventory_hostname }} is {{ hostvars[inventory_hostname]['public_ip_address'] }}"
          - "Ansible Play hosts {{ ansible_play_hosts }}"
          - "Ansible Private_ip_address {{ hostvars[inventory_hostname]['private_ip_address'] }}"
      ignore_errors: true

    - name: Print SSH connection
      debug:
        msg: "Connecting to {{ inventory_hostname }} via {{ ansible_host }} \
          using {{ ansible_ssh_common_args | default('No SSH args set')  }}"
      ignore_errors: true

    ### Setup Proxy and install the needed packages for the test servers.
    ### All of the servers have zfs set for use, but needs to set activeated
    - name:  Add in the DNF update and install role
      ansible.builtin.include_role:
        name: Dnf_access_install

    ### find teh data drives nvme1n1, and for teh OSS nvme1n2
    - name: Print ZFS Status
      ansible.builtin.shell:
        cmd: "lsblk | grep [35]0."
      register: Data_disks
      become: yes
      become_user: root
      

    - name: Is zfs on the systems? No, but here is the lsblk info
      ansible.builtin.debug:
        msg: "What is there: {{ Data_disks }}"

    ### A place to store temporary and extra data for Lustre on alll servers
    - name: Create mound point for data scratch drive
      ansible.builtin.file:
        path: "{{ mount_point }}"
        state: directory
        mode: '0755'
      become: yes
      become_user: root

    ### for data drives create ex4 file system
    - name: Create File System
      filesystem:
        fstype: ext4
        dev: "{{ data_device_name }}"
      become: yes
      become_user: root

    - name: Mount File System
      mount:
        path: "{{ mount_point }}"
        src: "{{ data_device_name }}"
        fstype: ext4
        state: mounted
      become: yes
      become_user: root

    ### set the three servers with data drives
    - name: Create mount points for data drives
      when: "ansible_host == '10.0.1.11'" 
      block:
        - name: Create mound point for OSS server
          ansible.builtin.file:
            path: "{{ luster_point }}"
            state: directory
            mode: '0755'
          
        - name: start ZFS
          ansible.builtin.command: "/sbin/modprobe zfs"
          register: modprobe_zfs
        
        - name: Is zfs on the systems? No, but here is the lsblk info
          ansible.builtin.debug:
            msg: "What is there: {{ modprobe_zfs }}"


        - name: Create a new file system called myfs in pool rpool with the setuid property turned off
          community.general.zfs:
            name: rpool/myfs
            state: present
            extra_zfs_properties:
              setuid: 'off'

        - name: Mount Lustre EBS volume
          mount:
            path: "{{ mount_point }}"
            src: "{{ device_name }}"
            fstype: ext4
            state: mounted
      become: yes
      become_user: root
    ### ende


