---
  ### create zfs pools and setup drives

### A place to Lustre scrath area on alll servers
- name: Create mound point for data scratch drive
  ansible.builtin.file:
    path: "{{ lustre_scratch }}"
    state: directory
    mode: '0755'
  become: yes
  become_user: root

  ### Start both the zfs and lustre
- name: start ZFS modprobe
  ansible.builtin.command: "/sbin/modprobe zfs"
  register: modprobe_zfs 
  become: yes
  become_user: root

- name: start lustre modprobe
  ansible.builtin.command: "/sbin/modprobe lustre"
  register: modprobe_lustre
  become: yes
  become_user: root

  ### set the OSS driver with the 500 GB
- name: Create mount points for OST data drive
  when: "ansible_host == '10.0.1.11'" 
  block:
      ### the data pool for Lustre, for test a single drive
      ### A more reaistic environmient is a few servers (OSS) with RAID/JBOD of Data_disks.
    - name: Create mound point for OST server
      ansible.builtin.file:
        path: "{{ lustre_point }}"
        state: directory
        mode: '0755'
    
      ### check for active zfs (the base for Lustre)
    - name: Is modprobe
      ansible.builtin.debug:
        msg: 
          - "What is there zfs  {{ modprobe_zfs }}"
          - "What is there lustre: {{ modprobe_lustre }}"

      ### create the ZFS pool on the data drive #2 500
      ##  If the zpool is already set, it will error.  This command only needs to be run once per deploy
    # - name: Create OST zpool
    #   command: "zpool create {{ lustre_pool_name }} {{ lustre_device_name }}"

      # Create the OSS file system:
    - name: Create Lustre data store for OST
      command: "mkfs.lustre --backfstype=zfs --fsname={{ lustre_pool_name }} --ost --mgsnode= '10.0.1.10'@ec2-user  {{ lustre_pool_name }}"
      # mkfs.lustre --mdt --fsname=testfs --backfstype=zfs --mgsnode=... mdt_pool
      register: command_output
      failed_when: "command_output.rc != 0 and ('was previously formatted for lustre' not in command_output.stderr) \
        and command_output.rc != 17"
      changed_when: "command_output.rc == 0"

    - name: Mount Lustre data store for OST
      command: "mount -t lustre {{ lustre_device_name }} {{ lustre_point }}" 

  become: yes
  become_user: root
  ### Ende block

  ### MGT/MDS server with the 30 GB scratch drive
- name: Create mount points for MGT data drives
  when: "ansible_host == '10.0.1.10'" 
  block:

    - name: Is modprobe_mng_zfs
      ansible.builtin.debug:
        msg: 
          - "What is there zfs  {{ modprobe_zfs }}"
          - "What is there lustre: {{ modprobe_lustre }}"

      ### create the ZFS pool on the data scratch drives
      ### this will teh lustre scratch for MNG
      ##  If the zpool is already set, it will error.  This command only needs to be run once per deploy
    # - name: Create MGT zpool
    #   command: "zpool create {{ lustre_scratch_name }} {{ data_device_name }}"
    #   register: command_output
    #   failed_when: "command_output.rc != 0 and ('is already mounted' not in command_output.stderr)"
    #   changed_when: "command_output.rc == 0"

      # Create the MGS/MDT file system:
    - name: Create Luster data store for MGS/MDT
      command: "mkfs.lustre --backfstype=zfs --fsname={{ lustre_scratch_name }} --mgs --mdt {{ lustre_pool_name }}"
      # mkfs.lustre --mdt --fsname=testfs --backfstype=zfs --mgsnode=... mdt_pool
      register: command_output
      failed_when: "command_output.rc != 0 and ('was previously formatted for lustre' not in command_output.stderr) \
        and command_output.rc != 17"
      changed_when: "command_output.rc == 0"

    #   ### Set the scratch drive for zfs
    # - name: Create a new file MGT in pool rpool with the setuid property turned off
    #   community.general.zfs:
    #     name: "{{ lustre_scratch_name }}"
    #     state: present
    #     extra_zfs_properties:
    #       setuid: 'off'

    - name: Mount Lustre data store for OST
      command: "mount -t lustre {{ lustre_scratch }} {{ lustre_scratch_name }}" 

    #   ### For only OSS, the others are later
    # - name: Is Lustre installed and ready?
    #   ansible.builtin.command: "/sbin/modprobe -v lustre"
    #   register: modprobe_lustre
  become: yes
  become_user: root
  ### Ende block

### With teh zf drives set now run Lustre, even the client 
### Start the luster utility on all systems, 
- name: start Lustre - all servers
  ansible.builtin.command: "/sbin/modprobe lustre"
  register: modprobe_ost_lustre