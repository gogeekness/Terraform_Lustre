### main Lustre playbook
---
# Main Lustre Playbook

- name: Setup Lustre on test servers 
  hosts: all
  become: true
  become_user: ec2-user
  gather_facts: false

  # stop ansible from hanging
  any_errors_fatal: true
  # stop ansible if can not log on.
  ignore_unreachable: true

  #### set roles for playbook
  ## update python to python3.8
  roles:
    - role: Set_SSH_Vars

  vars:
    data_device_name: "/dev/nvme1n1"  # Lust
    lustre_device_name: "/dev/nvme2n1" # ZFS drive for Lustre device_name = "/dev/sdz"
    lustre_pool_name: "Lust_pool"
    lustre_point: "/mnt/lustre"
    lustre_scratch: "/mnt/lustre_scratch"
    lustre_scratch_name: "Lust_scratch"
    ansible_command_timeout: 60  # timeout if ansible gets stuck
    mounted_devices: "{{ ansible_mounts | map(attribute='device') }}"

  pre_tasks:
  
  ### Set vault file and fetch secret vars from it
    - name: Include vault
      ansible.builtin.include_vars:
        file: Terraform_Vault

  tasks:
    ### I need to key to be on the bastion host so it can copy file to the other servers.
    - name: Copy ssh-key to non-bastion hosts
      ansible.builtin.copy:
        src: "{{ ssh_key_location }}"
        dest: "/home/ec2-user/.ssh/lustretest"
        owner: ec2-user
        mode: '0600'
      when: inventory_hostname != bastion_host

    ### shwo Current IP ans host status
    - name: Print Public IP Address
      debug:
        msg:
          - "Public IP of {{ inventory_hostname }} is {{ hostvars[inventory_hostname]['public_ip_address'] }}"
          - "Ansible Play hosts {{ ansible_play_hosts }}"
          - "Ansible Private_ip_address {{ hostvars[inventory_hostname]['private_ip_address'] }}"
      ignore_errors: true

    - name: Print SSH connection
      debug:
        msg: "Connecting to {{ inventory_hostname }} via {{ ansible_host }} \
          using {{ ansible_ssh_common_args | default('No SSH args set')  }}"
      ignore_errors: true

    ### Setup Proxy and install the needed packages for the test servers.
    ### All of the servers have zfs set for use, but needs to set activeated
    - name:  Add in the DNF update and install role
      ansible.builtin.include_role:
        name: Dnf_access_install

    ### find the data drives nvme1n1, and for the OSS nvme1n2
    - name: Print ZFS Status
      ansible.builtin.shell:
        cmd: "lsblk | grep [35]0."
      register: Data_disks
      become: yes
      become_user: root
      

    - name: Is zfs on the systems? No, but here is the lsblk info
      ansible.builtin.debug:
        msg: "What is there: {{ Data_disks }}"

    ### A place to Lustre scrath area on alll servers
    - name: Create mound point for data scratch drive
      ansible.builtin.file:
        path: "{{ lustre_scratch }}"
        state: directory
        mode: '0755'
      become: yes
      become_user: root

    # ### for data drives create ex4 file system
    # - name: Create File System
    #   filesystem:
    #     fstype: ext4
    #     dev: "{{ data_device_name }}"
    #   become: yes
    #   become_user: root

    ### set the OSS driver with the 500 GB
    - name: Create mount points for data drives
      when: "ansible_host == '10.0.1.11'" 
      block:
          ### the data pool for Lustre, for test a single drive
          ### A more reaistic environmient is a few servers (OSS) with RAID/JBOD of Data_disks.
        - name: Create mound point for OSS server
          ansible.builtin.file:
            path: "{{ lustre_point }}"
            state: directory
            mode: '0755'
          
          ### As this is a command it will try to overwrite on a 2nd run.
        - name: start ZFS - OSS
          ansible.builtin.command: "/sbin/modprobe zfs"
          register: modprobe_oss_zfs
        
        - name: start Lustre - OSS
          ansible.builtin.command: "/sbin/modprobe lustre"
          register: modprobe_oss_lustre
        
          ### check for active zfs (the base for Lustre)
        - name: Is modprobe_oss_zfs
          ansible.builtin.debug:
            msg: 
              - "What is there zfs  {{ modprobe_oss_zfs }}"
              - "What is there lustre: {{ modprobe_oss_lustre }}"

          ### create the ZFS pool on the data drive #2 500
          ##  If the zpool is already set, it will error.  This command only needs to be run once per deploy
        - name: Create OSS zpool
          command: "zpool create {{ lustre_pool_name }} {{ lustre_device_name }}"
          ignore_errors: true  

        - name: Create a new file system called myfs in pool rpool with the setuid property turned off
          community.general.zfs:
            name: "{{ lustre_pool_name }}"
            state: present
            extra_zfs_properties:
              setuid: 'off'

      become: yes
      become_user: root
      ### Ende block
      

    ### MGT/MDS server with the 30 GB scratch drive
    - name: Create mount points for data drives
      when: "ansible_host == '10.0.1.10'" 
      block:
          ### As this is a command it will try to overwrite on a 2nd run.
        - name: start ZFS - MGT
          ansible.builtin.command: "/sbin/modprobe zfs"
          register: modprobe_mgt_zfs
        
          ### As this is a command it will try to overwrite on a 2nd run.
        - name: start Lustre  - MGT
          ansible.builtin.command: "/sbin/modprobe lustre"
          register: modprobe_mgt_lustre

        - name: Is modprobe_oss_zfs
          ansible.builtin.debug:
            msg: 
              - "What is there zfs  {{ modprobe_mgt_zfs }}"
              - "What is there lustre: {{ modprobe_mgt_lustre }}"

          ### create the ZFS pool on the data scratch drives
          ### this will teh lustre scratch for MNG
          ##  If the zpool is already set, it will error.  This command only needs to be run once per deploy
        - name: Create OSS zpool
          command: "zpool create {{ lustre_scratch_name }} {{ data_device_name }}"
          ignore_errors: true
          
          ### Set the scratch drive for zfs
        - name: Create a new file system called myfs in pool rpool with the setuid property turned off
          community.general.zfs:
            name: "{{ lustre_scratch_name }}"
            state: present
            extra_zfs_properties:
              setuid: 'off'

          ### For only OSS, the others are later
        - name: Is Lustre installed and ready?
          ansible.builtin.command: "/sbin/modprobe -v lustre"
          register: modprobe_lustre
      become: yes
      become_user: root
      ### Ende block




    # - name: Set up Lustre on the systems
    #   Create luster on OSS and MNGS

    # - name: Setup client 
    #   Create Lustre client 


    ### ende


