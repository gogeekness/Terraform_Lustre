### main Lustre playbook
---
# Main Lustre Playbook
hosts: lustre

become: true
become_user: ec2-user
gather_facts: false

vars:
  data_device_name: "/dev/xvdd"  # Lust 
  lustre_device_name: "dev/xvdz" # ZFS drive for Lustre
  lustre_pool_name: "Lust_pool"
  mount_point: "/mnt/data"

tasks:
  - name: Add SSH private key
    copy:
      content: "{{ lookup('env', 'SSH_PRIVATE_KEY') }}"
      dest: "/home/ec2-user/.ssh/rsa.id"
      owner: ec2-user
      group: ec2-user
      mode: '0600'

  - name: Install common packages
    package:
      name: 
        - util-linux
        - xfsprogs
        - nano
        - netools
      state: present

### set the three servers with data drives
  - name: Create mount points for data drives
    file:
      path: "{{ data_device_name }}"
      state: directory
      mode: '0755'
    with_sequence: start=1 end=3

  - name: Create filesystem on EBS volume
    filesystem:
      fstype: ext4
      dev: "{{ data_device_name }}"
    when: device_name is defined

  - name: Create mount directory
    file:
      path: "{{ mount_point }}"
      state: directory
      mode: '0755'

  - name: Mount EBS volume
    mount:
      path: "{{ mount_point }}"
      src: "{{ device_name }}"
      fstype: ext4
      state: mounted

  - name: Add SSH private key to point connection server
    copy:
      content: "{{ lookup('env', 'SSH_PRIVATE_KEY') }}"
      dest: "/home/ec2-user/.ssh/cluster_key"
      owner: ec2-user
      group: ec2-user
      mode: '0600'
    when: lookup('env', 'SSH_PRIVATE_KEY') is defined