### main Lustre playbook
---
# Main Lustre Playbook

- name: Setup Lustre on test servers 
  hosts: all
  become: true
  become_user: ec2-user
  gather_facts: false

  # stop ansible from hanging
  any_errors_fatal: true
  # stop ansible if can not log on.
  ignore_unreachable: true

  #### set roles for playbook
  ## update python to python3.8
  roles:
    - role: Set_SSH_Vars

  vars:
    data_device_name: "/dev/nvme1n1"  # Lust
    lustre_device_name: "/dev/nvme2n1" # ZFS drive for Lustre device_name = "/dev/sdz"
    lustre_pool_name: "Lustre"
    lustre_point: "/mnt/lustre"
    lustre_scratch: "/mnt/lustre_scratch"
    lustre_scratch_name: "Lust_scratch"
    ansible_command_timeout: 60  # timeout if ansible gets stuck
    mounted_devices: "{{ ansible_mounts | map(attribute='device') }}"

  pre_tasks:
  
  ### Set vault file and fetch secret vars from it
    - name: Include vault
      ansible.builtin.include_vars:
        file: Terraform_Vault

  tasks:
    ### I need to key to be on the bastion host so it can copy file to the other servers.
    - name: Copy ssh-key to non-bastion hosts
      ansible.builtin.copy:
        src: "{{ ssh_key_location }}"
        dest: "/home/ec2-user/.ssh/lustretest"
        owner: ec2-user
        mode: '0600'
      when: inventory_hostname != bastion_host

    ### shwo Current IP ans host status
    - name: Print Public IP Address
      debug:
        msg:
          - "Public IP of {{ inventory_hostname }} is {{ hostvars[inventory_hostname]['public_ip_address'] }}"
          - "Ansible Play hosts {{ ansible_play_hosts }}"
          - "Ansible Private_ip_address {{ hostvars[inventory_hostname]['private_ip_address'] }}"
      ignore_errors: true

    - name: Print SSH connection
      debug:
        msg: "Connecting to {{ inventory_hostname }} via {{ ansible_host }} \
          using {{ ansible_ssh_common_args | default('No SSH args set')  }}"
      ignore_errors: true

    ### Setup Proxy and install the needed packages for the test servers.
    ### All of the servers have zfs set for use, but needs to set activeated
    - name:  Add in the DNF update and install role
      ansible.builtin.include_role:
        name: Dnf_access_install

    ### find the data drives nvme1n1, and for the OSS nvme1n2
    - name: Print ZFS Status
      ansible.builtin.shell:
        cmd: "lsblk | grep [35]0."
      register: Data_disks
      become: yes
      become_user: root
      

    - name: Is zfs on the systems? No, but here is the lsblk info
      ansible.builtin.debug:
        msg: "What is there: {{ Data_disks }}"

    ### All of the servers have zfs set for use, Creaet zfs 
    - name:  Add in the zfs create role
      ansible.builtin.include_role:
        name: Create_zfs




    # ### for data drives create ex4 file system
    # - name: Create File System
    #   filesystem:
    #     fstype: ext4
    #     dev: "{{ data_device_name }}"
    #   become: yes
    #   become_user: root






    # - name: Set up Lustre on the systems
    #   Create luster on OSS and MNGS

    # - name: Setup client 
    #   Create Lustre client 


    ### ende


